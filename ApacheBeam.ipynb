{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cac7afee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install apache-beam[gcp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "382ccf1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/Users/umangsood/Library/Jupyter/runtime/kernel-7106d616-f6ed-4654-b868-d0351b9ff8bb.json']\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/Users/umangsood/Library/Jupyter/runtime/kernel-7106d616-f6ed-4654-b868-d0351b9ff8bb.json']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Element Count: 1000\n",
      "Average Processing Time: 23.0 milliseconds\n",
      "CPU Usage: 21.00%\n",
      "Memory Usage: 63.00%\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.metrics import Metrics, MetricsFilter\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "class ExtractAndProcess(beam.DoFn):\n",
    "    def __init__(self):\n",
    "        self.element_count = Metrics.counter(self.__class__, 'element_count')\n",
    "        self.processing_time = Metrics.distribution(self.__class__, 'processing_time')\n",
    "        self.cpu_usage = Metrics.gauge(self.__class__, 'cpu_usage')\n",
    "        self.memory_usage = Metrics.gauge(self.__class__, 'memory_usage')\n",
    "\n",
    "    def process(self, element):\n",
    "#         print(element)\n",
    "        # Skip the header row\n",
    "        \n",
    "        if element[0].startswith('date,max_temp,min_temp'):\n",
    "            return\n",
    "\n",
    "        # Start measuring processing time\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Measure CPU and memory usage before processing\n",
    "        cpu_before = psutil.cpu_percent()\n",
    "        memory_before = psutil.virtual_memory().percent\n",
    "        \n",
    "        for element in element:\n",
    "            if element.startswith('date,max_temp,min_temp'):\n",
    "                continue\n",
    "\n",
    "            date, max_temp, min_temp = element.split(',')\n",
    "            avg_temp = (float(max_temp) + float(min_temp)) / 2\n",
    "\n",
    "            # Emit the result\n",
    "            yield {'date': date, 'avg_temp': avg_temp}\n",
    "\n",
    "            # Increment element count metric\n",
    "            self.element_count.inc()\n",
    "\n",
    "        # Record processing time metric\n",
    "        end_time = time.time()\n",
    "        self.processing_time.update(int((end_time - start_time) * 1000))  # in milliseconds\n",
    "\n",
    "        # Measure CPU and memory usage after processing\n",
    "        cpu_after = psutil.cpu_percent()\n",
    "        memory_after = psutil.virtual_memory().percent\n",
    "\n",
    "        # Update CPU and memory usage metrics\n",
    "        self.cpu_usage.set(cpu_after)\n",
    "        self.memory_usage.set(memory_after)\n",
    "\n",
    "def run(argv=None):\n",
    "    # Define your pipeline options\n",
    "    pipeline_options = beam.pipeline.PipelineOptions(argv)\n",
    "\n",
    "    # Create a pipeline\n",
    "    with beam.Pipeline(options=pipeline_options) as p:\n",
    "        # Read the CSV file and create a subset of 100 elements\n",
    "        subset_lines = (\n",
    "            p\n",
    "            | 'Read CSV (Last 1000)' >> beam.io.ReadFromText('preprocessed_data.csv')\n",
    "            | 'Sample 1000' >> beam.transforms.combiners.Sample.FixedSizeGlobally(1000)\n",
    "        )\n",
    "\n",
    "        # Apply the ExtractAndProcess transform to the subset\n",
    "        results = (\n",
    "            subset_lines\n",
    "            | 'Extract and Process' >> beam.ParDo(ExtractAndProcess())\n",
    "            | 'Write Output' >> beam.io.WriteToText('output1000')\n",
    "        )\n",
    "\n",
    "    # Retrieve and print metrics\n",
    "    result = p.run()\n",
    "\n",
    "    # Accessing metrics directly without querying\n",
    "    element_count = result.metrics().query(MetricsFilter().with_name('element_count'))\n",
    "    processing_time = result.metrics().query(MetricsFilter().with_name('processing_time'))\n",
    "    cpu_usage = result.metrics().query(MetricsFilter().with_name('cpu_usage'))\n",
    "    memory_usage = result.metrics().query(MetricsFilter().with_name('memory_usage'))\n",
    "\n",
    "    # Print the metrics\n",
    "    if element_count:\n",
    "        print(f\"Total Element Count: {element_count['counters'][0].committed}\")\n",
    "    if processing_time:\n",
    "        print(f\"Average Processing Time: {processing_time['distributions'][0].committed.mean} milliseconds\")\n",
    "    if cpu_usage:\n",
    "        print(f\"CPU Usage: {cpu_usage['gauges'][0].committed.value:.2f}%\")\n",
    "    if memory_usage:\n",
    "        print(f\"Memory Usage: {memory_usage['gauges'][0].committed.value:.2f}%\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d614999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/Users/umangsood/Library/Jupyter/runtime/kernel-7106d616-f6ed-4654-b868-d0351b9ff8bb.json']\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/Users/umangsood/Library/Jupyter/runtime/kernel-7106d616-f6ed-4654-b868-d0351b9ff8bb.json']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Element Count: 5000\n",
      "Average Processing Time: 93.0 milliseconds\n",
      "CPU Usage: 24.00%\n",
      "Memory Usage: 63.00%\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.metrics import Metrics, MetricsFilter\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "class ExtractAndProcess(beam.DoFn):\n",
    "    def __init__(self):\n",
    "        self.element_count = Metrics.counter(self.__class__, 'element_count')\n",
    "        self.processing_time = Metrics.distribution(self.__class__, 'processing_time')\n",
    "        self.cpu_usage = Metrics.gauge(self.__class__, 'cpu_usage')\n",
    "        self.memory_usage = Metrics.gauge(self.__class__, 'memory_usage')\n",
    "\n",
    "    def process(self, element):\n",
    "#         print(element)\n",
    "        # Skip the header row\n",
    "        \n",
    "        if element[0].startswith('date,max_temp,min_temp'):\n",
    "            return\n",
    "\n",
    "        # Start measuring processing time\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Measure CPU and memory usage before processing\n",
    "        cpu_before = psutil.cpu_percent()\n",
    "        memory_before = psutil.virtual_memory().percent\n",
    "        \n",
    "        for element in element:\n",
    "            if element.startswith('date,max_temp,min_temp'):\n",
    "                continue\n",
    "\n",
    "            date, max_temp, min_temp = element.split(',')\n",
    "            avg_temp = (float(max_temp) + float(min_temp)) / 2\n",
    "\n",
    "            # Emit the result\n",
    "            yield {'date': date, 'avg_temp': avg_temp}\n",
    "\n",
    "            # Increment element count metric\n",
    "            self.element_count.inc()\n",
    "\n",
    "        # Record processing time metric\n",
    "        end_time = time.time()\n",
    "        self.processing_time.update(int((end_time - start_time) * 1000))  # in milliseconds\n",
    "\n",
    "        # Measure CPU and memory usage after processing\n",
    "        cpu_after = psutil.cpu_percent()\n",
    "        memory_after = psutil.virtual_memory().percent\n",
    "\n",
    "        # Update CPU and memory usage metrics\n",
    "        self.cpu_usage.set(cpu_after)\n",
    "        self.memory_usage.set(memory_after)\n",
    "\n",
    "def run(argv=None):\n",
    "    # Define your pipeline options\n",
    "    pipeline_options = beam.pipeline.PipelineOptions(argv)\n",
    "\n",
    "    # Create a pipeline\n",
    "    with beam.Pipeline(options=pipeline_options) as p:\n",
    "        # Read the CSV file and create a subset of 100 elements\n",
    "        subset_lines = (\n",
    "            p\n",
    "            | 'Read CSV (Last 5000)' >> beam.io.ReadFromText('preprocessed_data.csv')\n",
    "            | 'Sample 5000' >> beam.transforms.combiners.Sample.FixedSizeGlobally(5000)\n",
    "        )\n",
    "\n",
    "        # Apply the ExtractAndProcess transform to the subset\n",
    "        results = (\n",
    "            subset_lines\n",
    "            | 'Extract and Process' >> beam.ParDo(ExtractAndProcess())\n",
    "            | 'Write Output' >> beam.io.WriteToText('output5000')\n",
    "        )\n",
    "\n",
    "    # Retrieve and print metrics\n",
    "    result = p.run()\n",
    "\n",
    "    # Accessing metrics directly without querying\n",
    "    element_count = result.metrics().query(MetricsFilter().with_name('element_count'))\n",
    "    processing_time = result.metrics().query(MetricsFilter().with_name('processing_time'))\n",
    "    cpu_usage = result.metrics().query(MetricsFilter().with_name('cpu_usage'))\n",
    "    memory_usage = result.metrics().query(MetricsFilter().with_name('memory_usage'))\n",
    "\n",
    "    # Print the metrics\n",
    "    if element_count:\n",
    "        print(f\"Total Element Count: {element_count['counters'][0].committed}\")\n",
    "    if processing_time:\n",
    "        print(f\"Average Processing Time: {processing_time['distributions'][0].committed.mean} milliseconds\")\n",
    "    if cpu_usage:\n",
    "        print(f\"CPU Usage: {cpu_usage['gauges'][0].committed.value:.2f}%\")\n",
    "    if memory_usage:\n",
    "        print(f\"Memory Usage: {memory_usage['gauges'][0].committed.value:.2f}%\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f0a045a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/Users/umangsood/Library/Jupyter/runtime/kernel-7106d616-f6ed-4654-b868-d0351b9ff8bb.json']\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/Users/umangsood/Library/Jupyter/runtime/kernel-7106d616-f6ed-4654-b868-d0351b9ff8bb.json']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Element Count: 15000\n",
      "Average Processing Time: 228.0 milliseconds\n",
      "CPU Usage: 17.00%\n",
      "Memory Usage: 64.00%\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.metrics import Metrics, MetricsFilter\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "class ExtractAndProcess(beam.DoFn):\n",
    "    def __init__(self):\n",
    "        self.element_count = Metrics.counter(self.__class__, 'element_count')\n",
    "        self.processing_time = Metrics.distribution(self.__class__, 'processing_time')\n",
    "        self.cpu_usage = Metrics.gauge(self.__class__, 'cpu_usage')\n",
    "        self.memory_usage = Metrics.gauge(self.__class__, 'memory_usage')\n",
    "\n",
    "    def process(self, element):\n",
    "#         print(element)\n",
    "        # Skip the header row\n",
    "        \n",
    "        if element[0].startswith('date,max_temp,min_temp'):\n",
    "            return\n",
    "\n",
    "        # Start measuring processing time\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Measure CPU and memory usage before processing\n",
    "        cpu_before = psutil.cpu_percent()\n",
    "        memory_before = psutil.virtual_memory().percent\n",
    "        \n",
    "        for element in element:\n",
    "            if element.startswith('date,max_temp,min_temp'):\n",
    "                continue\n",
    "            date, max_temp, min_temp = element.split(',')\n",
    "            avg_temp = (float(max_temp) + float(min_temp)) / 2\n",
    "\n",
    "            # Emit the result\n",
    "            yield {'date': date, 'avg_temp': avg_temp}\n",
    "\n",
    "            # Increment element count metric\n",
    "            self.element_count.inc()\n",
    "\n",
    "        # Record processing time metric\n",
    "        end_time = time.time()\n",
    "        self.processing_time.update(int((end_time - start_time) * 1000))  # in milliseconds\n",
    "\n",
    "        # Measure CPU and memory usage after processing\n",
    "        cpu_after = psutil.cpu_percent()\n",
    "        memory_after = psutil.virtual_memory().percent\n",
    "\n",
    "        # Update CPU and memory usage metrics\n",
    "        self.cpu_usage.set(cpu_after)\n",
    "        self.memory_usage.set(memory_after)\n",
    "\n",
    "def run(argv=None):\n",
    "    # Define your pipeline options\n",
    "    pipeline_options = beam.pipeline.PipelineOptions(argv)\n",
    "\n",
    "    # Create a pipeline\n",
    "    with beam.Pipeline(options=pipeline_options) as p:\n",
    "        # Read the CSV file and create a subset of 100 elements\n",
    "        subset_lines = (\n",
    "            p\n",
    "            | 'Read CSV (Last 15000)' >> beam.io.ReadFromText('preprocessed_data.csv')\n",
    "            | 'Sample 15000' >> beam.transforms.combiners.Sample.FixedSizeGlobally(15001)\n",
    "        )\n",
    "\n",
    "        # Apply the ExtractAndProcess transform to the subset\n",
    "        results = (\n",
    "            subset_lines\n",
    "            | 'Extract and Process' >> beam.ParDo(ExtractAndProcess())\n",
    "            | 'Write Output' >> beam.io.WriteToText('output15000')\n",
    "        )\n",
    "\n",
    "    # Retrieve and print metrics\n",
    "    result = p.run()\n",
    "\n",
    "    # Accessing metrics directly without querying\n",
    "    element_count = result.metrics().query(MetricsFilter().with_name('element_count'))\n",
    "    processing_time = result.metrics().query(MetricsFilter().with_name('processing_time'))\n",
    "    cpu_usage = result.metrics().query(MetricsFilter().with_name('cpu_usage'))\n",
    "    memory_usage = result.metrics().query(MetricsFilter().with_name('memory_usage'))\n",
    "\n",
    "    # Print the metrics\n",
    "    if element_count:\n",
    "        print(f\"Total Element Count: {element_count['counters'][0].committed}\")\n",
    "    if processing_time:\n",
    "        print(f\"Average Processing Time: {processing_time['distributions'][0].committed.mean} milliseconds\")\n",
    "    if cpu_usage:\n",
    "        print(f\"CPU Usage: {cpu_usage['gauges'][0].committed.value:.2f}%\")\n",
    "    if memory_usage:\n",
    "        print(f\"Memory Usage: {memory_usage['gauges'][0].committed.value:.2f}%\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a3e8de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
